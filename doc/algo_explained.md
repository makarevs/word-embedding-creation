# Word Embeddings using a Simple Neural Network

1. **Data preparation**: The program constructs a dataset by creating pairs of main words and their context words (using a sliding window approach). Each word pair consists of a main word `w` and its corresponding context word `c`. After the pair-finding part of the algorithm finishes, there will be `num_pairs` (84 with our specific set of sentences in `input\sample.csv`).

2. **One-hot encoding**: For each word pair (main word `w` and context word `c`), the program creates a one-hot encoded input vector `X[w]` for the main word and a one-hot encoded output vector `Y[c]` for the context word. Both `X[w]` and `Y[c]` are binary vectors of length `n_words`. 
   
   ***Dimensionality explained***
   
   Let's clarify that when we say "word `w`" it means we are referring to the word in the `w-th` place in the sorted list of all words in the combined text. For example "word 7" is `king` found with index 7 in the list `words`. We will loosely denote as `X[w]` the one-hot-encoded vector corresponding to word `w`. Respectively, by convention of one-hot encoding, it will have length `n_words`, only the `w-th` element in it will have 1, while other elemnts having 0es. The overall X matrix consisting of such row-vectors will have shape (`num_pairs`, `n_words`). Since with our input data `input\sample.csv` we have 21 non-stop words betwen all sentences, we will have X matrix with 84 row and 21 columns, and Y with the same dimensionality. So, to clarify, `X[w]` in our loose notaion is indeed referring to the `p-th` row of X matrix, `X[p,:]`, where `p-th` pair contains `w` as the main word. There could be more than one pair that has `w` as main word, but all context words will be different between such pairs. Example: both 1st and 2nd pairs have `future` (index 6 in vocabulary) as the main word, but context words are `king` and `prince` in those pairs.

3. **Neural network architecture**: The program defines a simple feedforward neural network, processing one word pair (one X-Y pair) at a time. So all of the matrices and vectors below may be thought of having additional pairs-counting index `p` associating them with the specific pair `(X[p,:], Y[p,:])`, to which we loosely refer to as `(X[w], Y[c])` in order to focus on specific words `w` and `c`, rather than concentrating on the fact that each word may participate in more than one pair.
   - **Input layer**: Takes the one-hot encoded representation `X[w]` of the main word `w` (dimension: `n_words`).
   - **First Dense layer**: A linear dense layer with `embed_size` neurons that maps the one-hot encoded input vector `X[w]` into a lower-dimensional vector `Z[w]` (dimension: `embed_size`). This layer learns the word embeddings.
   - **Second Dense layer**: A dense layer with `n_words` neurons followed by a softmax activation function that predicts the probability distribution `P[c|w]` over context words `c` given the input word `w`.

   ***Dimensionality explained***
   
   Recall that when we say "of word `w`" it means we are referring to the `p-th` pair having `w` as the main word. `(X[p,:]` contains one-hot vector for `w` and `Y[p,:])` contains one-hot vector for `c`. The probability distribution `P[c|w]` is obtained from embedding `Z[w]` by multiplying it with weights W2, and it will be compared to `Y[p,:])`, since we will want it to contain (after training) the high probability number only for `c-th` element of the distribution vector.

4. **Training**: The neural network is trained on the one-hot encoded input vectors `X[w]` and output vectors `Y[c]` to optimize the weights (W1 and W2) to produce the most accurate probability distribution `P[c|w]` over context words `c` for each main word `w`. The training process utilizes the 'adam' optimizer and the categorical cross-entropy loss function. The aim is to make the softmax output closely resemble the one-hot encoded output vector `Y[c]` for each word pair, by encouraging the model to assign higher probabilities to the correct context words and lower probabilities to the incorrect ones.

5. **Embeddings extraction**: After training is complete, the program extracts the word embeddings as the weights of the first dense layer (W1)

   ***Dimensionality explained***
   
    W1 that has shape `(n_words, embed_size)`. So, to get embedding for main word `w` (say, in pair `p`) we take `w-th` row W1[w,:] in the W1. The embeddings are also the lower-dimensional `(embed_size,)` vectors `Z[w]` (think `p-th` row `Z[p,:]`), since that is what remains of W1 after multiplying by one-hot input vector `X[w]` (a.k.a. `(X[p,:]`), zeroes "masking" all other rows of W1.

At the core of the algorithm, the neural network processes each `(w, c)` word pair (or, rather, their corresponding one-hot vectors `(X[p,:], Y[p,:])`, to which we loosely refer to as `(X[w], Y[c])`) independently, learning to predict the probability distribution over vector of possible words, thought here about as playing the role of context words `c`, for a given main word `w`. The first dense layer (W1) maps the one-hot encoded main word vectors `X[w]` into lower-dimensional embedding space (yielding vectors `Z[w]`). The second dense layer (W2) learns to predict, based on `Z[w]`, the probability distribution over all `n_words` context words. During training, the weights of both W1 and W2 layers are optimized iteratively by minimizing the loss function (categorical cross-entropy) on a pair-by-pair basis to learn better embeddings and representations.

The weights of W2 can also be considered as context word embeddings in the embedding space. These embeddings represent the relationships between input word embeddings and the context word embeddings that are captured during the learning process. However, keep in mind that combining the `W1` and `W2` weights for word embeddings doesn't always yield better results. In the program provided, the architecture is simple, and their combination might not provide significant improvements. Nevertheless, more advanced models for word embeddings, such as Word2Vec's Skip-Gram and Continuous Bag of Words (CBOW) models or GloVe, capture relationships between words more explicitly and might benefit from utilizing or combining the weights of input and output word embeddings.